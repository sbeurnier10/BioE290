{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "a) df(x,y)/dx = 4x^3 - 2x + 2y\n",
    "\n",
    "df(x,y)/dy = 2y +2x\n",
    "\n",
    "D(x,y) = d^2f(x,y)/dx^2*d^2f(x,y)/dy^2 - [d^2f(x,y)/dxdy] = 24x^2 - 6\n",
    "\n",
    "d^2f(x,y)/dx^2 = 12x^2 - 2\n",
    "\n",
    "df(1,-1)/dx = 4 - 2 - 2 = 0   df(1,-1)/dy = -2 + 2 = 0\n",
    "D(1,-1) = 24 - 6 > 0\n",
    "d^2f(1,-1)/dx^2 = 12 - 2 > 0\n",
    "Relative Min at (1,-1)\n",
    "\n",
    "df(-1,1)/dx = -4 + 2 + 2 = 0\n",
    "df(-1,1)/dy = 0\n",
    "D(-1,1) = 18 > 0\n",
    "d^2f(-1,1)/dx^2 = 10 > 0\n",
    "Relative Min at (-1,1)\n",
    "\n",
    "df(0,0)/dx = 0\n",
    "df(0,0)/dy = 0\n",
    "D(0,0) = -6 < 0\n",
    "Saddle Point at (0,0)\n",
    "\n",
    "df(0,-2)/dx = -4\n",
    "There is no local min/man nor saddle point at (0,-2)\n",
    "\n",
    "b)\n",
    "gradf(x,y) = (4x^3-2x+2y,2x+2y)\n",
    "new(x,y) = old(x,y) - stepsize*gradf(old(x,y))\n",
    "new(x,y) = (1.5,1.5) - 0.1*(13.5,6) = (0.15,0.9)\n",
    "From part a, local minima are at (1,-1) and (-1,1) so this new point is closer to either of the minima then the last point. It is a good optimization step. I'd say to decay the stepsize/learning rate of steps as we get closer to either of the minima to ensure we don't overshoot the minima and to prevent the algorithm from running longer than it has to. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([-0.99999852,  0.99999607]),\n",
       " 'evaluation': -2.999999999985186,\n",
       " 'steps': 51,\n",
       " 'path': array([[ 1.5       ,  1.5       ],\n",
       "        [ 0.15      ,  0.9       ],\n",
       "        [-0.03162   ,  0.648     ],\n",
       "        [-0.22733235,  0.47048256],\n",
       "        [-0.4603766 ,  0.38644985],\n",
       "        [-0.73063964,  0.41710875],\n",
       "        [-0.91361447,  0.57314178],\n",
       "        [-0.89067253,  0.77647099],\n",
       "        [-0.98168777,  0.81739147],\n",
       "        [-0.94167921,  0.88803587],\n",
       "        [-1.0240441 ,  0.91571465],\n",
       "        [-0.95964931,  0.94925202],\n",
       "        [-1.01216804,  0.95311466],\n",
       "        [-0.98795692,  0.96627781],\n",
       "        [-0.99481157,  0.9720766 ],\n",
       "        [-0.99412388,  0.97937406],\n",
       "        [-0.99741632,  0.98505533],\n",
       "        [-0.99646125,  0.99076871],\n",
       "        [-1.00111334,  0.9939261 ],\n",
       "        [-0.99723697,  0.99631795],\n",
       "        [-1.00126535,  0.99668496],\n",
       "        [-0.99895278,  0.99778247],\n",
       "        [-0.99981882,  0.99811897],\n",
       "        [-0.99948229,  0.99870549],\n",
       "        [-1.00001741,  0.99902712],\n",
       "        [-0.99975409,  0.99927314],\n",
       "        [-0.99990384,  0.99941651],\n",
       "        [-0.99986709,  0.99959085],\n",
       "        [-0.99997668,  0.99970943],\n",
       "        [-0.99988705,  0.9998471 ],\n",
       "        [-1.00001432,  0.99985945],\n",
       "        [-0.99993563,  0.99991689],\n",
       "        [-0.99998875,  0.99992106],\n",
       "        [-0.99998269,  0.99993914],\n",
       "        [-0.99999092,  0.9999531 ],\n",
       "        [-0.99999034,  0.99996764],\n",
       "        [-0.9999977 ,  0.99997812],\n",
       "        [-0.99999196,  0.99998896],\n",
       "        [-1.00000165,  0.99998995],\n",
       "        [-0.99999435,  0.99999462],\n",
       "        [-0.99999982,  0.99999455]])}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c)\n",
    "import numpy as np\n",
    "\n",
    "def funcz(point):\n",
    "    x = point.item(0)\n",
    "    y = point.item(1)\n",
    "    return x**4 - x**2 + y**2 + 2*x*y - 2\n",
    "\n",
    "def deriv_z(point):\n",
    "    x = point.item(0)\n",
    "    y = point.item(1)\n",
    "    gradx = 4*x**3 - 2*x + 2*y\n",
    "    grady = 2*y +2*x\n",
    "    return(np.array([gradx,grady]))\n",
    "\n",
    "\n",
    "from pylab import *\n",
    "import numpy.linalg as LA\n",
    "\n",
    "    \n",
    "\n",
    "def steepest_descent(func,first_derivate,starting_point,stepsize,tol):\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv=first_derivate(starting_point)\n",
    "    count=0\n",
    "    visited=[]\n",
    "    while LA.norm(deriv)>tol and count<1e6:\n",
    "        # calculate new point position\n",
    "        new_point=starting_point-stepsize*deriv\n",
    "        if func(new_point)<func(starting_point):\n",
    "            visited.append(starting_point)\n",
    "            starting_point=new_point\n",
    "            deriv=first_derivate(starting_point)\n",
    "            stepsize*=1.2\n",
    "            count+=1\n",
    "        else:\n",
    "            stepsize/=2\n",
    "            count+=1\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"steps\":count,\"path\":np.asarray(visited)}\n",
    "\n",
    "point1 = np.array([1.5,1.5])\n",
    "\n",
    "steepest_descent(funcz,deriv_z,point1,0.1,1e-5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 51 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 26\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n"
     ]
    }
   ],
   "source": [
    "#Part d)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "min = minimize(funcz, point1, method='CG', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})\n",
    "\n",
    "min2 = minimize(funcz, point1, method='BFGS', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both BFGS and CG are more efficient at finding a local minimum for the function as they took less steps, or iterations (7 and 9 respectively), than steepest descent (51). This is further emphasized in the return reports as BFGS evaluted the function and the gradient fewer times than steepest descent, and CG computed the gradient fewer times than steepest descent (it did evaluate the function more but not by much).\n",
    "BFGS is also more efficient than CG for this function, taking less iterations and having to evaluate less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([0.99999089, 0.99998153]),\n",
       " 'evaluation': 8.361266797324188e-11,\n",
       " 'steps': 1523,\n",
       " 'path': array([[-0.5       ,  1.5       ],\n",
       "        [-1.05      ,  0.875     ],\n",
       "        [-0.845175  ,  0.94325   ],\n",
       "        ...,\n",
       "        [ 0.99999084,  0.99998106],\n",
       "        [ 0.99999068,  0.99998135],\n",
       "        [ 0.99999093,  0.99998135]])}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def banana(point):\n",
    "    x = point.item(0)\n",
    "    y = point.item(1)\n",
    "    return (1-x)**2 + 10*(y-x**2)**2\n",
    "\n",
    "def deriv_b(point):\n",
    "    x = point.item(0)\n",
    "    y = point.item(1)\n",
    "    gradx = -2*(1-x) - 10*2*(y-x**2)*2*x\n",
    "    grady = 10*2*(y-x**2)\n",
    "    return np.array([gradx,grady])\n",
    "\n",
    "point2 = np.array([-0.5,1.5])\n",
    "\n",
    "steepest_descent(banana,deriv_b,point2,0.1,1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 1523 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([0.99998962, 0.99997877]),\n",
       " 'evaluation': 1.0990925662589922e-10,\n",
       " 'count': 1422,\n",
       " 'path': array([[-0.5       ,  1.5       ],\n",
       "        [-1.15265669,  1.00609769],\n",
       "        [-0.82644891,  1.14971199],\n",
       "        ...,\n",
       "        [ 0.99998932,  0.99997846],\n",
       "        [ 0.9999897 ,  0.9999785 ],\n",
       "        [ 0.99998944,  0.9999787 ]])}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b)\n",
    "def stochastic_gradient_descent(func,first_derivate,starting_point,stepsize,tol=1e-5,stochastic_injection=0.2):\n",
    "    deriv=first_derivate(starting_point)\n",
    "    count=0\n",
    "    visited=[]\n",
    "    while LA.norm(deriv)>tol and count<1e5:\n",
    "        if stochastic_injection>0:\n",
    "            random_vector=np.random.random(len(starting_point))*2-1\n",
    "            stochastic_deriv=LA.norm(deriv)*random_vector/LA.norm(random_vector)\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        new_point=starting_point+stepsize*(direction)\n",
    "\n",
    "        if func(new_point)<func(starting_point):\n",
    "            visited.append(starting_point)\n",
    "            starting_point=new_point\n",
    "            deriv=first_derivate(starting_point)\n",
    "            stepsize*=1.2\n",
    "            count+=1\n",
    "            \n",
    "        else:\n",
    "            stepsize/=2\n",
    "            count+=1\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"count\":count,\"path\":np.asarray(visited)}\n",
    "\n",
    "stochastic_gradient_descent(banana,deriv_b,point2,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 20\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 22\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 31\n"
     ]
    }
   ],
   "source": [
    "#c)\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "min = minimize(banana, point2, method='CG', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})\n",
    "\n",
    "min2 = minimize(banana, point2, method='BFGS', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BFGS and CG are better than SGD, only having to go through about 20 iterations to get to the mininum whereas SGD takes about 1400 steps (and still only reaches a point very close to the minimum but not the minimum itself as is the case for the former two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)\n",
    "A firm outcome can be drawn from one run of BFGS and GC because these methods are not stochastic, i.e. they will output the same result (and steps to achieve the result) every time they are ran.\n",
    "On the other hand, a firm outcome cannot be drawn from one run of SGD because, as its name indicates, there is some randomness involved, and thus it will output a different result (and a different set of steps to achieve the result) every time it is ran. However, the degree to which these results and steps differ is up to the value of the stochastic injection, so for runs in which the stochastic injection is relatively small in order to find the minimum, the results and steps done will, on average, be quite similar for each run using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 30\n",
      "         Function evaluations: 231\n",
      "         Gradient evaluations: 77\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 52\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 66\n"
     ]
    }
   ],
   "source": [
    "point3 = np.array([2,2])\n",
    "point4 = np.array([2,5])\n",
    "point5 = np.array([12,45])\n",
    "\n",
    "stochastic_gradient_descent(banana,deriv_b,point5,0.1)\n",
    "\n",
    "min = minimize(banana, point5, method='CG', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})\n",
    "\n",
    "min2 = minimize(banana, point5, method='BFGS', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, CG and BFGS definitely perform better on the Rosenbrock Banana function than SGD. For different points, the number of iterations on both CG and BFGS did not change much, (even at a point like (12,45) the number of iterations needed was less than 3 times the original number of iterations needed for (-0.5,1.5)). For SGD on the other hand, it reached the same result as the two other algorithms but the number of iterations needed seemed to increase at a larger factor with farther points (the number of iterations needed starting at (12,45) was about 30 times larger than the number of iterations needed starting at (-0.5,1.5)). This indicates that non-stochastic methods do better on the Banana Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 21\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 31\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 38\n"
     ]
    }
   ],
   "source": [
    "#a)\n",
    "#ð‘“(ð‘¥,ð‘¦)=2ð‘¥2 âˆ’1.05ð‘¥4 +ð‘¥6 6â„ +ð‘¥ð‘¦+ð‘¦2 \n",
    "\n",
    "def camel(point):\n",
    "    x = point.item(0)\n",
    "    y = point.item(1)\n",
    "    return 2*x**2 - 1.05*x**4 + x**6/6 + x*y + y**2\n",
    "\n",
    "def deriv_c(point):\n",
    "    x = point.item(0)\n",
    "    y = point.item(1)\n",
    "    gradx = 4*x - 4.2*x**3 + x**5 + y\n",
    "    grady = y + 2*y\n",
    "    return np.array([gradx,grady])\n",
    "\n",
    "pointf = np.array([-1.5,-1.5])\n",
    "\n",
    "stochastic_gradient_descent(camel,deriv_c,pointf,0.1)\n",
    "\n",
    "min = minimize(camel, pointf, method='CG', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})\n",
    "\n",
    "min2 = minimize(banana, pointf, method='BFGS', #jac=rosen_der,\n",
    "               options={'gtol': 1e-5, 'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, BFGS and CG still did better than SGD. About half the time, SGD would take 15-25 steps to find the minimum (which is less than BFGS), but the half of the the time it would take the max number (10,000) to \"converge\" to the minimum. BFGS and CG constistently took 7 and 31 steps for this starting point, making them more efficient on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([-1.13039054e-06, -2.11839905e-06]),\n",
       " 'evaluation': 9.437798298729542e-12,\n",
       " 'count': 254,\n",
       " 'path': array([[-1.50000000e+00, -1.50000000e+00],\n",
       "        [-1.06222625e+00, -7.47849256e-01],\n",
       "        [-1.19718534e-01,  3.44714000e-01],\n",
       "        [-1.19719341e-01,  3.44695958e-01],\n",
       "        [-1.19716464e-01,  3.44654845e-01],\n",
       "        [-1.19724725e-01,  3.44595024e-01],\n",
       "        [-1.19718549e-01,  3.44505951e-01],\n",
       "        [-1.19723952e-01,  3.44378543e-01],\n",
       "        [-1.19745466e-01,  3.44231665e-01],\n",
       "        [-1.19789996e-01,  3.44034169e-01],\n",
       "        [-1.19865092e-01,  3.43777893e-01],\n",
       "        [-1.19961504e-01,  3.43495845e-01],\n",
       "        [-1.20092969e-01,  3.43177911e-01],\n",
       "        [-1.20282864e-01,  3.42791391e-01],\n",
       "        [-1.20487758e-01,  3.42373975e-01],\n",
       "        [-1.20736268e-01,  3.41770751e-01],\n",
       "        [-1.20912846e-01,  3.40966484e-01],\n",
       "        [-1.21204716e-01,  3.39981645e-01],\n",
       "        [-1.21638326e-01,  3.38754333e-01],\n",
       "        [-1.21953860e-01,  3.37159307e-01],\n",
       "        [-1.22433534e-01,  3.35348344e-01],\n",
       "        [-1.22701009e-01,  3.33057929e-01],\n",
       "        [-1.22666449e-01,  3.30266927e-01],\n",
       "        [-1.22900386e-01,  3.27018619e-01],\n",
       "        [-1.22775514e-01,  3.22863348e-01],\n",
       "        [-1.22912974e-01,  3.17582731e-01],\n",
       "        [-1.22452194e-01,  3.11091365e-01],\n",
       "        [-1.21274833e-01,  3.03074143e-01],\n",
       "        [-1.19109385e-01,  2.93444032e-01],\n",
       "        [-1.15837952e-01,  2.81663124e-01],\n",
       "        [-1.12185578e-01,  2.68957342e-01],\n",
       "        [-1.07102650e-01,  2.54868743e-01],\n",
       "        [-1.00006865e-01,  2.38681082e-01],\n",
       "        [-9.28590037e-02,  2.20980363e-01],\n",
       "        [-8.63687872e-02,  2.00784819e-01],\n",
       "        [-7.80630460e-02,  1.75520700e-01],\n",
       "        [-6.93708527e-02,  1.48141641e-01],\n",
       "        [-6.10185335e-02,  1.17865685e-01],\n",
       "        [-4.95586778e-02,  8.41011629e-02],\n",
       "        [-3.80712399e-02,  4.54148815e-02],\n",
       "        [-2.60548201e-02,  3.12844142e-03],\n",
       "        [-2.25339463e-02, -8.53193327e-03],\n",
       "        [-2.25335069e-02, -8.53126903e-03],\n",
       "        [-2.25329763e-02, -8.53008605e-03],\n",
       "        [-2.25309769e-02, -8.52825106e-03],\n",
       "        [-2.25274052e-02, -8.52680437e-03],\n",
       "        [-2.25233170e-02, -8.52568449e-03],\n",
       "        [-2.25168332e-02, -8.52306032e-03],\n",
       "        [-2.25083406e-02, -8.51822804e-03],\n",
       "        [-2.24979054e-02, -8.51435080e-03],\n",
       "        [-2.24867091e-02, -8.50958422e-03],\n",
       "        [-2.24698705e-02, -8.50555683e-03],\n",
       "        [-2.24517794e-02, -8.50065104e-03],\n",
       "        [-2.24250123e-02, -8.49230300e-03],\n",
       "        [-2.23891169e-02, -8.47798413e-03],\n",
       "        [-2.23439447e-02, -8.46671197e-03],\n",
       "        [-2.22836091e-02, -8.44813371e-03],\n",
       "        [-2.22173651e-02, -8.42153027e-03],\n",
       "        [-2.21300547e-02, -8.38289766e-03],\n",
       "        [-2.20162557e-02, -8.32994677e-03],\n",
       "        [-2.18915668e-02, -8.27682600e-03],\n",
       "        [-2.17430881e-02, -8.23174869e-03],\n",
       "        [-2.15457724e-02, -8.16935481e-03],\n",
       "        [-2.13309966e-02, -8.10646931e-03],\n",
       "        [-2.10999375e-02, -8.03096645e-03],\n",
       "        [-2.08354202e-02, -7.91257964e-03],\n",
       "        [-2.05470533e-02, -7.75759542e-03],\n",
       "        [-2.02268842e-02, -7.54943389e-03],\n",
       "        [-1.97731416e-02, -7.31528734e-03],\n",
       "        [-1.91599124e-02, -7.06758644e-03],\n",
       "        [-1.83991635e-02, -6.85184548e-03],\n",
       "        [-1.74607966e-02, -6.51777813e-03],\n",
       "        [-1.63834955e-02, -6.03940724e-03],\n",
       "        [-1.50087519e-02, -5.53269256e-03],\n",
       "        [-1.34811400e-02, -5.05128950e-03],\n",
       "        [-1.18299880e-02, -4.53617402e-03],\n",
       "        [-9.80821954e-03, -3.85331051e-03],\n",
       "        [-7.41017809e-03, -3.19242357e-03],\n",
       "        [-4.68658667e-03, -2.56989163e-03],\n",
       "        [-1.65191772e-03, -1.76176943e-03],\n",
       "        [ 1.63575065e-03, -8.20420027e-04],\n",
       "        [ 1.63575051e-03, -8.20420344e-04],\n",
       "        [ 1.63566717e-03, -8.20377477e-04],\n",
       "        [ 1.63548468e-03, -8.20269830e-04],\n",
       "        [ 1.63528601e-03, -8.20096670e-04],\n",
       "        [ 1.63491780e-03, -8.19859071e-04],\n",
       "        [ 1.63452781e-03, -8.19571025e-04],\n",
       "        [ 1.63390796e-03, -8.19140841e-04],\n",
       "        [ 1.63321277e-03, -8.18715933e-04],\n",
       "        [ 1.63235966e-03, -8.18030792e-04],\n",
       "        [ 1.63118978e-03, -8.17402717e-04],\n",
       "        [ 1.62986446e-03, -8.16809974e-04],\n",
       "        [ 1.62837329e-03, -8.15851879e-04],\n",
       "        [ 1.62675112e-03, -8.14518201e-04],\n",
       "        [ 1.62436953e-03, -8.13249261e-04],\n",
       "        [ 1.62175208e-03, -8.11401998e-04],\n",
       "        [ 1.61883691e-03, -8.08790486e-04],\n",
       "        [ 1.61531252e-03, -8.05029614e-04],\n",
       "        [ 1.61061016e-03, -8.01482167e-04],\n",
       "        [ 1.60375763e-03, -7.96644365e-04],\n",
       "        [ 1.59479038e-03, -7.89794020e-04],\n",
       "        [ 1.58521440e-03, -7.81677606e-04],\n",
       "        [ 1.57428912e-03, -7.73646214e-04],\n",
       "        [ 1.55959295e-03, -7.66000978e-04],\n",
       "        [ 1.53904353e-03, -7.57386370e-04],\n",
       "        [ 1.51699843e-03, -7.47630683e-04],\n",
       "        [ 1.48781291e-03, -7.38435191e-04],\n",
       "        [ 1.45656743e-03, -7.26274125e-04],\n",
       "        [ 1.42304889e-03, -7.12480625e-04],\n",
       "        [ 1.37710169e-03, -6.98610156e-04],\n",
       "        [ 1.32607606e-03, -6.76375153e-04],\n",
       "        [ 1.26905817e-03, -6.43684338e-04],\n",
       "        [ 1.19189652e-03, -6.08987082e-04],\n",
       "        [ 1.09454373e-03, -5.59252962e-04],\n",
       "        [ 9.85534543e-04, -4.92271932e-04],\n",
       "        [ 8.68657217e-04, -4.16760531e-04],\n",
       "        [ 7.26517743e-04, -3.19174477e-04],\n",
       "        [ 5.70185922e-04, -2.18191710e-04],\n",
       "        [ 3.97234344e-04, -1.14907252e-04],\n",
       "        [ 1.91966412e-04,  2.45514181e-06],\n",
       "        [-3.12098082e-05,  1.25221444e-04],\n",
       "        [-3.12089396e-05,  1.25214778e-04],\n",
       "        [-3.12089802e-05,  1.25199562e-04],\n",
       "        [-3.12042638e-05,  1.25179304e-04],\n",
       "        [-3.12008768e-05,  1.25157174e-04],\n",
       "        [-3.12010743e-05,  1.25120369e-04],\n",
       "        [-3.11931646e-05,  1.25069967e-04],\n",
       "        [-3.11747183e-05,  1.25003431e-04],\n",
       "        [-3.11527223e-05,  1.24931391e-04],\n",
       "        [-3.11428097e-05,  1.24843540e-04],\n",
       "        [-3.11494608e-05,  1.24733039e-04],\n",
       "        [-3.11408137e-05,  1.24579140e-04],\n",
       "        [-3.11151035e-05,  1.24406841e-04],\n",
       "        [-3.10616166e-05,  1.24175223e-04],\n",
       "        [-3.09680468e-05,  1.23885608e-04],\n",
       "        [-3.09016956e-05,  1.23555258e-04],\n",
       "        [-3.08514361e-05,  1.23100067e-04],\n",
       "        [-3.07907041e-05,  1.22608126e-04],\n",
       "        [-3.07015497e-05,  1.22072997e-04],\n",
       "        [-3.06163644e-05,  1.21319815e-04],\n",
       "        [-3.04644065e-05,  1.20487790e-04],\n",
       "        [-3.01778631e-05,  1.19491813e-04],\n",
       "        [-2.99829111e-05,  1.18361772e-04],\n",
       "        [-2.96565439e-05,  1.16830046e-04],\n",
       "        [-2.94692199e-05,  1.14838689e-04],\n",
       "        [-2.91699651e-05,  1.12212605e-04],\n",
       "        [-2.85553203e-05,  1.09122429e-04],\n",
       "        [-2.78268558e-05,  1.05113891e-04],\n",
       "        [-2.72032068e-05,  1.00745481e-04],\n",
       "        [-2.61749717e-05,  9.58692768e-05],\n",
       "        [-2.46749476e-05,  9.04418927e-05],\n",
       "        [-2.27784493e-05,  8.34784023e-05],\n",
       "        [-2.12367522e-05,  7.56855187e-05],\n",
       "        [-1.91249032e-05,  6.60063155e-05],\n",
       "        [-1.69842729e-05,  5.41079657e-05],\n",
       "        [-1.51072249e-05,  4.10633852e-05],\n",
       "        [-1.29869674e-05,  2.69832240e-05],\n",
       "        [-1.09851969e-05,  1.15639560e-05],\n",
       "        [-8.88320285e-06, -5.54886472e-06],\n",
       "        [-8.88300223e-06, -5.54907615e-06],\n",
       "        [-8.88255488e-06, -5.54869627e-06],\n",
       "        [-8.88117761e-06, -5.54851592e-06],\n",
       "        [-8.87962596e-06, -5.54844294e-06],\n",
       "        [-8.87641565e-06, -5.54774972e-06],\n",
       "        [-8.87301503e-06, -5.54673865e-06],\n",
       "        [-8.86889076e-06, -5.54619541e-06],\n",
       "        [-8.86412917e-06, -5.54607539e-06],\n",
       "        [-8.85860070e-06, -5.54652262e-06],\n",
       "        [-8.85124274e-06, -5.54822054e-06],\n",
       "        [-8.83859581e-06, -5.54958245e-06],\n",
       "        [-8.81949568e-06, -5.55111494e-06],\n",
       "        [-8.79611451e-06, -5.54793682e-06],\n",
       "        [-8.76983626e-06, -5.54599464e-06],\n",
       "        [-8.73794640e-06, -5.54690571e-06],\n",
       "        [-8.70122953e-06, -5.54051559e-06],\n",
       "        [-8.65633251e-06, -5.53795684e-06],\n",
       "        [-8.60523933e-06, -5.52509881e-06],\n",
       "        [-8.53064571e-06, -5.50403436e-06],\n",
       "        [-8.44404151e-06, -5.48765843e-06],\n",
       "        [-8.32294716e-06, -5.45994513e-06],\n",
       "        [-8.19237843e-06, -5.41698701e-06],\n",
       "        [-8.04894794e-06, -5.37480735e-06],\n",
       "        [-7.85011187e-06, -5.33101969e-06],\n",
       "        [-7.63226524e-06, -5.28915083e-06],\n",
       "        [-7.33285428e-06, -5.23409176e-06],\n",
       "        [-6.99749629e-06, -5.18962582e-06],\n",
       "        [-6.58164843e-06, -5.07888644e-06],\n",
       "        [-6.13771644e-06, -4.93756472e-06],\n",
       "        [-5.63862156e-06, -4.70989469e-06],\n",
       "        [-4.99487922e-06, -4.38762525e-06],\n",
       "        [-4.28586667e-06, -3.95420343e-06],\n",
       "        [-3.39743273e-06, -3.40280366e-06],\n",
       "        [-2.30491903e-06, -2.78976463e-06]])}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SGDM(func,first_derivate,starting_point,stepsize,momentum=0,tol=1e-5,stochastic_injection=1):\n",
    "    deriv=first_derivate(starting_point)\n",
    "    count=0\n",
    "    visited=[]\n",
    "    previous_direction=np.zeros(len(starting_point))\n",
    "    while LA.norm(deriv)>tol and count<1e5:\n",
    "        if stochastic_injection>0:\n",
    "            random_vector=np.random.random(len(starting_point))*2-1\n",
    "            stochastic_deriv=LA.norm(deriv)*random_vector/LA.norm(random_vector)\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        new_point=starting_point+stepsize*(momentum*previous_direction+direction)\n",
    "\n",
    "        if func(new_point)<func(starting_point):\n",
    "            visited.append(starting_point)\n",
    "            starting_point=new_point\n",
    "            previous_direction=direction+momentum*previous_direction\n",
    "            deriv=first_derivate(starting_point)\n",
    "            stepsize*=1.2\n",
    "            count+=1\n",
    "            \n",
    "        else:\n",
    "            if stepsize<1e-5:\n",
    "                previous_direction=previous_direction-previous_direction\n",
    "            else:\n",
    "                stepsize/=2\n",
    "            count+=1\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"count\":count,\"path\":np.asarray(visited)}\n",
    "\n",
    "SGDM(camel,deriv_c,pointf,0.1,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, SGDM did better than SGD (taking between 250 and 300 steps whereas SGD took 100,000 half the time). However, it still did not do better than CG or BFGS which took 7 and 31 iterations respecitvely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for drawing optimization path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def draw_path(func,path,x_min=-2,x_max=2,y_min=-2,y_max=2):\n",
    "    a=np.linspace(x_min,x_max,100)\n",
    "    b=np.linspace(y_min,y_max,100)\n",
    "    x,y=np.meshgrid(a,b)\n",
    "    z=func((x,y))\n",
    "    fig,ax=plt.subplots()\n",
    "    my_contour=ax.contour(x,y,z,50)\n",
    "    plt.colorbar(my_contour)\n",
    "    ax.plot(path[:,0],path[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
