{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10\n",
    "\n",
    "### Today's Topic:\n",
    "* Batch Normalization\n",
    "* Residual Neural Network\n",
    "* Pytorch utilizing GPU speedup \n",
    "* MGCF cluster resources for running gpu calculations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scalin\n",
    "Documentation: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html <br>\n",
    "expected input of size (N, C, H, W) <br>\n",
    "the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100, 35, 45])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "inp = torch.randn(20, 100, 35, 45)\n",
    "bn = nn.BatchNorm2d(100)\n",
    "output=bn(inp)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1,6,kernel_size=3,padding=1), #before pooling (B,6,32,32)\n",
    "                                  nn.Conv2d(6,24,kernel_size=3,padding=1), # (B,24,16,16)\n",
    "                                  nn.Conv2d(24,12,kernel_size=5)]) # (B,12,4,4)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc = nn.ModuleList([nn.Linear(192,192),nn.Linear(192,10)])\n",
    "        self.activation = nn.ReLU()\n",
    "        self.bn = [nn.BatchNorm2d(6),nn.BatchNorm2d(24),nn.BatchNorm2d(12)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(2):\n",
    "            x = self.pooling(self.activation(self.bn[i](self.conv[i](x))))\n",
    "        x = nn.Flatten()(self.activation(self.bn[2](self.conv[2](x))))\n",
    "        x = self.activation(self.fc[0](x))\n",
    "        x = nn.Softmax(dim=-1)(self.fc[1](x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(6, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(24, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  )\n",
      "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): ModuleList(\n",
      "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (1): Linear(in_features=192, out_features=10, bias=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(torch.randn(20,1,32,32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive vs concatenative skip connections\n",
    "\n",
    "![](Additive-skip-connections-vs-concatenative-skip-connections-Rectangles-represent-data.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1,6,kernel_size=3,padding=1), #before pooling (B,6,32,32)\n",
    "                                  nn.Conv2d(6,24,kernel_size=3,padding=1), # (B,24,16,16)\n",
    "                                  nn.Conv2d(24,12,kernel_size=5)]) # (B,12,4,4)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc = nn.ModuleList([nn.Linear(192,192),nn.Linear(192,10)])\n",
    "        self.activation = nn.ReLU()\n",
    "        self.bn = nn.ModuleList([nn.BatchNorm2d(6),nn.BatchNorm2d(24),nn.BatchNorm2d(12)])\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        residual = inp\n",
    "        x = self.bn[0](self.conv[0](inp))\n",
    "        x = x+residual\n",
    "        x = self.pooling(self.activation(x))\n",
    "        x = self.pooling(self.activation(self.bn[1](self.conv[1](x))))\n",
    "        x = nn.Flatten()(self.activation(self.bn[2](self.conv[2](x))))\n",
    "        res2 = x\n",
    "        y = self.fc[0](x)\n",
    "        y = y+res2\n",
    "        y = self.activation(y)\n",
    "        y = nn.Softmax(dim=-1)(self.fc[1](y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(6, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(24, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  )\n",
      "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): ModuleList(\n",
      "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (1): Linear(in_features=192, out_features=10, bias=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0631, 0.1010, 0.1147, 0.0946, 0.1355, 0.1112, 0.0405, 0.0585, 0.2029,\n",
       "         0.0780],\n",
       "        [0.0475, 0.0992, 0.0883, 0.0634, 0.1551, 0.0577, 0.0239, 0.0528, 0.1920,\n",
       "         0.2201],\n",
       "        [0.1259, 0.1470, 0.1011, 0.1420, 0.1098, 0.0932, 0.0559, 0.0546, 0.0928,\n",
       "         0.0776],\n",
       "        [0.0746, 0.1018, 0.1461, 0.0732, 0.0977, 0.1013, 0.0859, 0.0697, 0.1884,\n",
       "         0.0614],\n",
       "        [0.0670, 0.0988, 0.0753, 0.1209, 0.1242, 0.1377, 0.0501, 0.0726, 0.1303,\n",
       "         0.1231],\n",
       "        [0.0525, 0.1411, 0.1190, 0.0990, 0.1273, 0.1060, 0.0420, 0.0347, 0.1723,\n",
       "         0.1063],\n",
       "        [0.0653, 0.1611, 0.1133, 0.1292, 0.1492, 0.1292, 0.0426, 0.0285, 0.1235,\n",
       "         0.0581],\n",
       "        [0.0657, 0.1586, 0.0416, 0.1459, 0.1279, 0.0620, 0.0569, 0.1083, 0.1045,\n",
       "         0.1286],\n",
       "        [0.0398, 0.0838, 0.1870, 0.1211, 0.1329, 0.0888, 0.0661, 0.0714, 0.1463,\n",
       "         0.0629],\n",
       "        [0.1144, 0.1105, 0.0934, 0.0829, 0.0905, 0.0736, 0.0770, 0.0598, 0.1404,\n",
       "         0.1575],\n",
       "        [0.0556, 0.1564, 0.1097, 0.1186, 0.1219, 0.0695, 0.0311, 0.0322, 0.2105,\n",
       "         0.0945],\n",
       "        [0.0857, 0.0972, 0.0891, 0.1482, 0.0982, 0.1419, 0.0476, 0.1025, 0.1282,\n",
       "         0.0613],\n",
       "        [0.0645, 0.1598, 0.0980, 0.1306, 0.0793, 0.0965, 0.0500, 0.0840, 0.1730,\n",
       "         0.0644],\n",
       "        [0.0830, 0.1129, 0.2034, 0.1204, 0.0772, 0.0651, 0.0738, 0.0685, 0.0909,\n",
       "         0.1047],\n",
       "        [0.0549, 0.1088, 0.1373, 0.1459, 0.0741, 0.1095, 0.0635, 0.0421, 0.1310,\n",
       "         0.1329],\n",
       "        [0.0528, 0.1206, 0.1106, 0.0596, 0.0577, 0.0662, 0.0419, 0.1930, 0.2004,\n",
       "         0.0973],\n",
       "        [0.0377, 0.0880, 0.2329, 0.0613, 0.2038, 0.1104, 0.0270, 0.0435, 0.1391,\n",
       "         0.0563],\n",
       "        [0.0835, 0.1577, 0.0770, 0.0944, 0.1537, 0.0929, 0.0289, 0.1097, 0.1078,\n",
       "         0.0943],\n",
       "        [0.0914, 0.0932, 0.1368, 0.1045, 0.1147, 0.1366, 0.0416, 0.0574, 0.1634,\n",
       "         0.0603],\n",
       "        [0.0981, 0.0808, 0.1493, 0.1601, 0.1109, 0.0543, 0.0444, 0.0730, 0.1020,\n",
       "         0.1271]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CNN()\n",
    "print(net)\n",
    "net(torch.randn(20, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using GPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking available resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of GPUs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A100-SXM4-40GB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move tensors to gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the tensors are generated on the CPU. Even the model is initialized on the CPU. Thus one has to manually ensure that the operations are done using GPU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.FloatTensor([0., 1., 2.])\n",
    "X_train.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a common PyTorch practice to initialize a variable, usually named device that will hold the device we’re training on (CPU or GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.to(device)\n",
    "X_train.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same logic applies to the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv): ModuleList(\n",
       "    (0): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(6, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(24, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): ModuleList(\n",
       "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (1): Linear(in_features=192, out_features=10, bias=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move tensors back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.cpu()\n",
    "X_train.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([0., 1., 2.])\n",
    "Y = torch.FloatTensor([0., 1., 2.])\n",
    "X = X.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57288/2223898888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "X+Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2., 4.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X+Y).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGCF cluster resources\n",
    "https://docs.google.com/document/d/1lIkJ6g772Ss5e-4CJ_xGjlVRfOVUq6gYnyGiEhtBc-Q/edit?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.9.0",
   "language": "python",
   "name": "pytorch-1.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
